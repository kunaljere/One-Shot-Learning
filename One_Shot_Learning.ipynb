{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZFc0gcBa3ev"
      },
      "source": [
        "## Imports and initial data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzLy82Mza5P9"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os, tqdm, numpy as np, copy\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWKTivQqrztZ",
        "outputId": "4041baba-dff7-45eb-dd01-7089ce16e1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nufcGOKtzsB",
        "outputId": "a95a2b69-5e91-479a-e74b-a16928010606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDqz3Feua6I6",
        "outputId": "a89d8027-cfde-4ddc-8cfb-fa4a79f84c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "aYO0vpgoa708",
        "outputId": "2956a030-83f2-4b3a-b328-40a5b9264484"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ede0fa4b-0eeb-431b-8a1a-9390e4ac76eb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ede0fa4b-0eeb-431b-8a1a-9390e4ac76eb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving omniglot.zip to omniglot.zip\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()\n",
        "!unzip omniglot.zip >/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEauMXmDHFiq"
      },
      "source": [
        "Get all of the filepaths of the images and their corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJBCQZ9eC2O5"
      },
      "outputs": [],
      "source": [
        "label = 0\n",
        "filepath_index = 0\n",
        "\n",
        "label_to_character = {} # stores the character corresponding to each class label (int)\n",
        "filepath_to_filepath_index = {} # stores the filepath index (int) corresponding to each filepath -- index used to reduce memory usage\n",
        "filepath_index_to_filepath = {} # convert back\n",
        "label_to_filepath_indices = {} # stores the list of image filepath indices for each class label\n",
        "\n",
        "# loop through all the images_background directories\n",
        "for root, dirs, files in os.walk('images_background/images_background'):\n",
        "    if len(files) > 0:\n",
        "\n",
        "        # record the character and label\n",
        "        label_to_character[label] = ''.join(root.split('/')[-2:])\n",
        "\n",
        "        # append all the filepaths to the corresponding label's list\n",
        "        label_to_filepath_indices[label] = []\n",
        "        for file_i in files:\n",
        "            filepath = str(root) + '/' + file_i\n",
        "            filepath_to_filepath_index[filepath] = filepath_index\n",
        "            filepath_index_to_filepath[filepath_index] = filepath\n",
        "            label_to_filepath_indices[label].append(filepath_index)\n",
        "            filepath_index += 1\n",
        "    label += 1\n",
        "\n",
        "# same as above for eval data\n",
        "for root, dirs, files in os.walk('images_evaluation/images_evaluation'):\n",
        "    if len(files) > 0:\n",
        "        label_to_character[label] = ''.join(root.split('/')[-2:])\n",
        "        label_to_filepath_indices[label] = []\n",
        "        for file_i in files:\n",
        "            filepath = str(root) + '/' + file_i\n",
        "            filepath_to_filepath_index[filepath] = filepath_index\n",
        "            filepath_index_to_filepath[filepath_index] = filepath\n",
        "            label_to_filepath_indices[label].append(filepath_index)\n",
        "            filepath_index += 1\n",
        "    label += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_oFdzYh2u_l"
      },
      "outputs": [],
      "source": [
        "# len(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E6n7YUGG0jb"
      },
      "source": [
        "Separate out the test classes and the train classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcRyxunqEYKT"
      },
      "outputs": [],
      "source": [
        "# extract labels and characters\n",
        "dict_items = label_to_character.items()\n",
        "labels = [dict_item[0] for dict_item in dict_items]\n",
        "characters = [dict_item[1] for dict_item in dict_items]\n",
        "\n",
        "# identify which characters are the first of their language\n",
        "is_first = np.array([character.split('character')[1] == '01' for character in characters])\n",
        "# get the labels of the last 5 such characters (i.e. identify one character for each of the last 5 languages) for each of test and validation\n",
        "valid_labels = [labels[i] for i in np.where(is_first)[0][-10:-5]]\n",
        "test_labels = [labels[i] for i in np.where(is_first)[0][-5:]]\n",
        "train_labels = [label for label in labels if ((label not in test_labels) and (label not in valid_labels))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhKIkvi4bVqx"
      },
      "outputs": [],
      "source": [
        "# converts images to tensors (to be used later)\n",
        "converter = transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjt7hkRELMg_"
      },
      "source": [
        "Functions to sample random pairs or triplets. Return a tuple, where the first element is the filepath indices and the second element is the class labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOcJENEobW0s"
      },
      "outputs": [],
      "source": [
        "def sample_pairs(num_same, num_diff):\n",
        "    pairs = [] # store the filepath indices for the image pairs: [[index1, index2], [index3, index4], ...]\n",
        "    labels = [] # store the labels for the image pairs: [[label1, label2], [label3, label4], ...]\n",
        "\n",
        "    for i in range(num_same):\n",
        "        same_label = np.random.choice(train_labels) # choose a random label\n",
        "        pairs.append(np.random.choice(list(label_to_filepath_indices[same_label]), 2, replace=False)) # extract path indices for a random pair within this label\n",
        "        labels.append(np.array([same_label, same_label])) # add the target labels\n",
        "\n",
        "    for i in range(num_diff):\n",
        "        diff_labels = np.random.choice(list(label_to_filepath_indices.keys()), 2, replace=False) # choose two random labels\n",
        "        pairs.append([np.random.choice(list(label_to_filepath_indices[label])) for label in diff_labels]) # extract a random path index from each label\n",
        "        labels.append(diff_labels)\n",
        "\n",
        "    return pairs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dbWDQCXwHUz"
      },
      "outputs": [],
      "source": [
        "def sample_triplets(num_triplets):\n",
        "    triplets = [] # store the filepath indices for the image triplets: [[index1, index2, index3], [index4, index5, index6], ...]\n",
        "    labels = [] # store the labels for the image triplets: [[label1, label2, label3], [label4, label5, label6], ...]\n",
        "\n",
        "    for i in range(num_triplets):\n",
        "        [same_label, diff_label] = np.random.choice(train_labels, 2, replace=False) # choose two random labels, one for the same and one for the different\n",
        "        indices = list(np.random.choice(list(label_to_filepath_indices[same_label]), 2, replace=False)) # extract two random path indices for the same label\n",
        "        indices.append(np.random.choice(list(label_to_filepath_indices[diff_label]))) # extract one random path index for the different label, and add to the list\n",
        "\n",
        "        triplets.append(indices) # add this triplet's path indices\n",
        "        labels.append([same_label, same_label, diff_label]) # add this triplet's target labels\n",
        "\n",
        "    return triplets, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoCXC5LYbXtL"
      },
      "source": [
        "## Set up the dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUvxixV0baoX"
      },
      "outputs": [],
      "source": [
        "class Omniglot_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, filepath_sets, label_sets, device, triplets=False):\n",
        "\n",
        "        self.filepath_sets = filepath_sets\n",
        "        self.label_sets = label_sets\n",
        "        self.device = device\n",
        "        self.triplets = triplets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filepath_sets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return {\"paths\":self.filepath_sets[idx], \"labels\":self.label_sets[idx]}\n",
        "\n",
        "def collate_fn(data):\n",
        "\n",
        "    # get the images from the data\n",
        "    image_sets = [[Image.open(filepath_index_to_filepath[e['paths'][i]]) for e in data] for i in range(len(data[0]['paths']))]\n",
        "    # convert the images to tensors\n",
        "    image_tensors = [torch.cat([converter(image).unsqueeze(0) for image in image_set], dim=0).to(device) for image_set in image_sets]\n",
        "    # extract the label for each image\n",
        "    character_indices = [torch.tensor([e['labels'][i] for e in data]).to(device) for i in range(len(data[0]['labels']))]\n",
        "\n",
        "    # turn on autograd\n",
        "    [image_tensor.requires_grad_(True) for image_tensor in image_tensors]\n",
        "\n",
        "    return {\"image_tensors\":(image_tensors), \"labels\":(character_indices)}\n",
        "    # return {\"image_tensors\":(images_tensor1.to(device), images_tensor2.to(device)), \"character_indices\":(character_indices1.to(device), character_indices2.to(device))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8PXMj9dbbkz"
      },
      "source": [
        "## Defining S-subtract and S-concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSa7J0webglr"
      },
      "outputs": [],
      "source": [
        "class SiameseNet(nn.Module):\n",
        "    def __init__(self, num_channels, kernel_sizes, pool_sizes, emb_dims, conv_dropout, linear_dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_channels = num_channels\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.pool_sizes = pool_sizes\n",
        "        self.emb_dims = emb_dims\n",
        "        self.conv_dropout = conv_dropout\n",
        "        self.linear_dropout = linear_dropout\n",
        "\n",
        "        size = 105\n",
        "        for i in range(len(self.kernel_sizes)):\n",
        "            size -= self.kernel_sizes[i]\n",
        "            size /= self.pool_sizes[i]\n",
        "            size = np.ceil(size)\n",
        "        linear_in = int(size)*int(size)*self.num_channels[-1]\n",
        "\n",
        "\n",
        "        # add convolution and pooling layers\n",
        "        self.conv_layers = nn.ModuleList([nn.Conv2d(1, self.num_channels[0], self.kernel_sizes[0])])\n",
        "        self.batch_norms = nn.ModuleList([nn.BatchNorm2d(self.num_channels[0])])\n",
        "        self.pool_layers = nn.ModuleList([nn.MaxPool2d(self.pool_sizes[0])])\n",
        "        for i in range(1, len(self.num_channels)):\n",
        "            self.conv_layers.append(nn.Conv2d(self.num_channels[i-1], self.num_channels[i], self.kernel_sizes[i]))\n",
        "            self.batch_norms.append(nn.BatchNorm2d(self.num_channels[i]))\n",
        "            self.pool_layers.append(nn.MaxPool2d(self.pool_sizes[i]))\n",
        "        self.dropout2d = nn.Dropout2d(p=self.conv_dropout)\n",
        "\n",
        "        # add linear layers\n",
        "        self.linear_layers = nn.ModuleList([nn.Linear(linear_in, self.emb_dims[0])])\n",
        "        for i in range(1, len(self.emb_dims)):\n",
        "            self.linear_layers.append(nn.Linear(self.emb_dims[i-1], self.emb_dims[i]))\n",
        "        self.dropout = nn.Dropout(p=self.linear_dropout)\n",
        "        self.similarity = nn.Linear(self.emb_dims[-1], 1)\n",
        "\n",
        "\n",
        "    # performs the embedding on a single branch of the siamese net\n",
        "    def embed_forward(self, image):\n",
        "\n",
        "        for conv, norm, pool in zip(self.conv_layers, self.batch_norms, self.pool_layers):\n",
        "            image = self.dropout2d(image)\n",
        "            image = conv(image)\n",
        "            image = F.relu(image)\n",
        "            image = norm(image)\n",
        "            image = pool(image)\n",
        "\n",
        "\n",
        "        image = torch.flatten(image, start_dim=1)\n",
        "\n",
        "        for linear in self.linear_layers:\n",
        "            image = self.dropout(image)\n",
        "            image = linear(image)\n",
        "            image = F.relu(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    # measures similarity based on two embeddings\n",
        "    def similarity_forward(self, embeds):\n",
        "\n",
        "        #Subtract Embeddings\n",
        "        diff = torch.abs(embeds[0] - embeds[1])\n",
        "        diff = self.dropout(diff)\n",
        "        similarity = F.sigmoid(self.similarity(diff).squeeze())\n",
        "\n",
        "        return similarity\n",
        "\n",
        "\n",
        "    # performs the embedding on both halves of the data, then measure similarity\n",
        "    # images is a tuple : (images1, images2)\n",
        "    def forward(self, images):\n",
        "\n",
        "        embeds = self.embed_forward(images[0]), self.embed_forward(images[1])\n",
        "        similarity = self.similarity_forward(embeds)\n",
        "\n",
        "        return embeds, similarity\n",
        "\n",
        "\n",
        "    def forward_triplets(self, images):\n",
        "\n",
        "        embed1, embed2, embed3 = self.embed_forward(images[0]), self.embed_forward(images[1]), self.embed_forward(images[2])\n",
        "        similarity_same = self.similarity_forward((embed1, embed2))\n",
        "        similarity_diff = self.similarity_forward((embed1, embed3))\n",
        "\n",
        "        return (embed1, embed2, embed3), (similarity_same, similarity_diff)\n",
        "\n",
        "\n",
        "    # computes the loss by performing a forward pass and then comparing to labels\n",
        "    def compute_loss(self, images, targets):\n",
        "\n",
        "        embeds, similarity = self.forward_triplets(images)\n",
        "        target_similarity_same = (targets[0] == targets[1]).float()\n",
        "        target_similarity_diff = (targets[0] == targets[2]).float()\n",
        "\n",
        "        loss_same = F.binary_cross_entropy(similarity[0], target_similarity_same)\n",
        "        loss_diff = F.binary_cross_entropy(similarity[1], target_similarity_diff)\n",
        "\n",
        "        loss = (loss_same/2 + loss_diff/2)/2\n",
        "        return embeds, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cxAhErAKL-L"
      },
      "outputs": [],
      "source": [
        "class SiameseNetConcat(SiameseNet):\n",
        "\n",
        "    def __init__(self, num_channels, kernel_sizes, pool_sizes, emb_dims, conv_dropout, linear_dropout):\n",
        "\n",
        "        super().__init__(num_channels, kernel_sizes, pool_sizes, emb_dims, conv_dropout, linear_dropout)\n",
        "\n",
        "        # define new combine layers\n",
        "        self.combine = nn.Linear(self.emb_dims[-1]*2, self.emb_dims[-1])\n",
        "\n",
        "    def similarity_forward_permutation(self, concat):\n",
        "\n",
        "        concat = self.dropout(concat)\n",
        "        concat = self.combine(concat)\n",
        "        concat = F.relu(concat)\n",
        "        concat = self.dropout(concat)\n",
        "\n",
        "        return F.sigmoid(self.similarity(concat)).squeeze()\n",
        "\n",
        "\n",
        "    def similarity_forward(self, embeds):\n",
        "\n",
        "        if self.training:\n",
        "            similarity_1 = self.similarity_forward_permutation(torch.cat((embeds[0], embeds[1]), 1))\n",
        "            similarity_2 = self.similarity_forward_permutation(torch.cat((embeds[1], embeds[0]), 1))\n",
        "\n",
        "            return similarity_1, similarity_2\n",
        "\n",
        "        else:\n",
        "            similarity = self.similarity_forward_permutation(torch.cat((embeds[0], embeds[1]), 1))\n",
        "            return similarity\n",
        "\n",
        "\n",
        "    def compute_loss(self, images, targets):\n",
        "\n",
        "        embeds, similarity = self.forward_triplets(images)\n",
        "        target_similarity_same = (targets[0] == targets[1]).float()\n",
        "        target_similarity_diff = (targets[0] == targets[2]).float()\n",
        "\n",
        "        loss_same = F.binary_cross_entropy(similarity[0][0], target_similarity_same)\n",
        "        loss_same += F.binary_cross_entropy(similarity[0][1], target_similarity_same)\n",
        "        loss_diff = F.binary_cross_entropy(similarity[1][0], target_similarity_diff)\n",
        "        loss_diff += F.binary_cross_entropy(similarity[1][1], target_similarity_diff)\n",
        "\n",
        "        loss = (loss_same/2 + loss_diff/2)/2\n",
        "        return embeds, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNPTsju1eTbI"
      },
      "source": [
        "## Defining S-multires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z03d2-fy8nTc"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1, conv_dropout=0):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        # print(in_channels)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "\n",
        "        self.dropout = nn.Dropout2d(p=conv_dropout)\n",
        "        self.conv = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, stride, self.padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(self.out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # print(\"Output Dimension: \", x.shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, num_conv_layers, conv_dropout, linear_dropout):\n",
        "\n",
        "      super().__init__()\n",
        "      self.emb_dims = 1024\n",
        "\n",
        "      # num_channels = [32, 64, 128, 256, 128, 64, 32, 8]\n",
        "      num_channels = [8] * num_conv_layers\n",
        "\n",
        "\n",
        "\n",
        "      self.conv1 = nn.ModuleList([ConvBlock(1, num_channels[0], kernel_size=9, padding=4, conv_dropout=conv_dropout)])\n",
        "      self.conv2 = nn.ModuleList([ConvBlock(1, num_channels[0], kernel_size=7, padding=3, conv_dropout=conv_dropout)])\n",
        "      self.conv3 = nn.ModuleList([ConvBlock(1, num_channels[0], kernel_size=5, padding=2, conv_dropout=conv_dropout)])\n",
        "      self.conv4 = nn.ModuleList([ConvBlock(1, num_channels[0], kernel_size=3, padding=1, conv_dropout=conv_dropout)])\n",
        "\n",
        "      for i in range(1, len(num_channels)):\n",
        "        self.conv1.append(ConvBlock(num_channels[i-1]*4, num_channels[i], kernel_size=9, padding=4, conv_dropout=conv_dropout))\n",
        "        self.conv2.append(ConvBlock(num_channels[i-1]*4, num_channels[i], kernel_size=7, padding=3, conv_dropout=conv_dropout))\n",
        "        self.conv3.append(ConvBlock(num_channels[i-1]*4, num_channels[i], kernel_size=5, padding=2, conv_dropout=conv_dropout))\n",
        "        self.conv4.append(ConvBlock(num_channels[i-1]*4, num_channels[i], kernel_size=3, padding=1, conv_dropout=conv_dropout))\n",
        "      # print(len(self.conv1))\n",
        "\n",
        "      self.similarity_layer = nn.Sequential(\n",
        "                            nn.Dropout2d(p=conv_dropout),\n",
        "                            nn.Conv2d(64,512,3),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Flatten(),\n",
        "                            nn.Dropout(p=linear_dropout),\n",
        "                            nn.Linear(512*103*103,1))\n",
        "\n",
        "  def embed_forward(self, image):\n",
        "\n",
        "      x_cat = torch.clone(image)\n",
        "      for i in range(len(self.conv1)):\n",
        "        # print(self.conv1[i].out_channels)\n",
        "\n",
        "        x1 = self.conv1[i].forward(x_cat)\n",
        "\n",
        "        x2 = self.conv2[i].forward(x_cat)\n",
        "\n",
        "        x3 = self.conv3[i].forward(x_cat)\n",
        "\n",
        "        x4 = self.conv4[i].forward(x_cat)\n",
        "\n",
        "        x_cat = torch.cat([x1, x2, x3, x4], dim=1)\n",
        "\n",
        "      # print(\"Output Concat Shape: \", x_cat.shape)\n",
        "\n",
        "      return x_cat\n",
        "\n",
        "  def similarity_forward(self, embeds):\n",
        "\n",
        "      if self.training:\n",
        "\n",
        "          #Concatenate Embeddings\n",
        "          concat1 = torch.cat((embeds[0], embeds[1]), 1)\n",
        "          similarity1 = F.sigmoid(self.similarity_layer(concat1)).squeeze()\n",
        "\n",
        "          concat2 = torch.cat((embeds[1], embeds[0]), 1)\n",
        "          similarity2 = F.sigmoid(self.similarity_layer(concat2)).squeeze()\n",
        "\n",
        "          return similarity1, similarity2\n",
        "\n",
        "      else:\n",
        "          concat = torch.cat((embeds[0], embeds[1]), 1)\n",
        "          return F.sigmoid(self.similarity_layer(concat)).squeeze()\n",
        "\n",
        "      return similarity\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "\n",
        "      embed1, embed2 = self.embed_forward(images[0]), self.embed_forward(images[1])\n",
        "      embeds = (embed1, embed2)\n",
        "      similarity = self.similarity_forward(embeds)\n",
        "\n",
        "      return embeds, similarity\n",
        "\n",
        "\n",
        "  def forward_triplets(self, images):\n",
        "\n",
        "    embed1, embed2, embed3 = self.embed_forward(images[0]), self.embed_forward(images[1]), self.embed_forward(images[2])\n",
        "    similarity_same = self.similarity_forward((embed1, embed2))\n",
        "    similarity_diff = self.similarity_forward((embed1, embed3))\n",
        "\n",
        "    return (embed1, embed2, embed3), (similarity_same, similarity_diff)\n",
        "\n",
        "\n",
        "  def compute_loss(self, images, targets):\n",
        "\n",
        "      embeds, similarity = self.forward_triplets(images)\n",
        "      target_similarity_same = (targets[0] == targets[1]).float()\n",
        "      target_similarity_diff = (targets[0] == targets[2]).float()\n",
        "\n",
        "      loss_same = F.binary_cross_entropy(similarity[0][0], target_similarity_same)\n",
        "      loss_same += F.binary_cross_entropy(similarity[0][1], target_similarity_same)\n",
        "      loss_diff = F.binary_cross_entropy(similarity[1][0], target_similarity_diff)\n",
        "      loss_diff += F.binary_cross_entropy(similarity[1][1], target_similarity_diff)\n",
        "\n",
        "      loss = (loss_same/2 + loss_diff/2)/2\n",
        "      return embeds, loss\n",
        "\n",
        "# image1 = torch.randn(1, 1, 105,105)\n",
        "# image2 = torch.randn(1, 1, 105,105)\n",
        "# # print(image.shape)\n",
        "# embed1, embed2 = multires.embed_forward(image1), multires.embed_forward(image1)\n",
        "# # print(embed.shape)\n",
        "# similarity = multires.similarity_forward((embed1, embed2))\n",
        "# print(similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdLA1lczbt8l"
      },
      "source": [
        "## Train and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzrRx3wQ8kGt"
      },
      "outputs": [],
      "source": [
        "def make_classifications(model, num_classes, num_trials, batch_size, labels):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        acc, loss = 0,0\n",
        "\n",
        "        for batch in range(int(np.ceil(num_trials/batch_size))):\n",
        "            batch_size = min((batch + 1) * batch_size, num_trials) - batch * batch_size\n",
        "\n",
        "            # store whether each one-shot trial is correct, and the loss incurred\n",
        "            correct = torch.zeros(batch_size, dtype=torch.bool).to(device)\n",
        "            loss = 0\n",
        "\n",
        "            # get validation labels\n",
        "            shuffled_labels = np.array([np.random.permutation(labels) for i in range(batch_size)])\n",
        "\n",
        "            # the true similarity (last element is always match)\n",
        "            true_similarity = torch.zeros((batch_size, num_classes)).to(device)\n",
        "            true_similarity[:,-1] = 1\n",
        "\n",
        "            exemplar_tensor = []\n",
        "            inference_tensor = []\n",
        "\n",
        "            i = 0\n",
        "            while i < batch_size:\n",
        "\n",
        "                ### Select 1 exemplar from each of 5 random characters; select a 2nd of the last character to classify ###\n",
        "                # get one random filepath index for each of the first 4 validation labels\n",
        "                exemplar_filepath_indices = [np.random.choice(label_to_filepath_indices[label], replace=False) for label in shuffled_labels[i,:-1]]\n",
        "                # get two random filepath indices for the remaining validation label\n",
        "                [final_exemplar_filepath_indices, inference_filepath_index] = np.random.choice(label_to_filepath_indices[shuffled_labels[i,-1]], 2, replace=False)\n",
        "                # add one of the filepath indices for the last validation label to the other list\n",
        "                exemplar_filepath_indices.append(final_exemplar_filepath_indices)\n",
        "\n",
        "                ### Extract the images ###\n",
        "                # extract the exemplar images and concatenate along batch dimensino\n",
        "                exemplar_images = torch.cat([converter(Image.open(filepath_index_to_filepath[index])).unsqueeze(0) for index in exemplar_filepath_indices], dim=0).to(device)\n",
        "                # extract the inference image and replicate along batch dimension to be the same size as exemplar_images\n",
        "                inference_image = converter(Image.open(filepath_index_to_filepath[inference_filepath_index])).expand(exemplar_images.size()).to(device)\n",
        "\n",
        "                exemplar_tensor.append(exemplar_images)\n",
        "                inference_tensor.append(inference_image)\n",
        "\n",
        "                i += 1\n",
        "\n",
        "            # prepare tensors\n",
        "            exemplar_tensor = torch.cat(exemplar_tensor).to(device)\n",
        "            inference_tensor = torch.cat(inference_tensor).to(device)\n",
        "\n",
        "            ### Compute the predicted similarities and rate the performance ###\n",
        "            _, similarities = model.forward((exemplar_tensor, inference_tensor))\n",
        "\n",
        "            similarities = similarities.view(batch_size, num_classes)\n",
        "            acc += sum([torch.argmax(similarities[row,:]) == num_classes-1 for row in range(batch_size)])/num_trials # correct label is always the last element\n",
        "            loss += F.binary_cross_entropy(similarities, true_similarity) * batch_size * num_classes / num_trials\n",
        "\n",
        "    return acc, loss # accuracy and average loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQslY6o6bvSX"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, model_file, num_epochs=200, lr=1e-3, valid_params=(5,320,32), concat_lr_ratio=1):\n",
        "\n",
        "    model.eval()\n",
        "    (num_classes, num_trials, valid_batch_size) = valid_params\n",
        "    valid_acc, valid_loss = make_classifications(model, num_classes, num_trials, valid_batch_size, valid_labels)\n",
        "    print(' %d-way, one-shot accuracy: %.1f%%. Loss: %f'%(num_classes, valid_acc*100, valid_loss))\n",
        "\n",
        "    concat_var_names = ['combine']\n",
        "    # setup\n",
        "    concat_named_params = list(filter(lambda kv: any(key in kv[0] for key in concat_var_names), model.named_parameters()))\n",
        "    not_concat_named_params = list(filter(lambda kv: not any(key in kv[0] for key in concat_var_names), model.named_parameters()))\n",
        "    concat_params = [e[1] for e in concat_named_params]\n",
        "    not_concat_params = [e[1] for e in not_concat_named_params]\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': not_concat_params},\n",
        "        {\n",
        "            'params': concat_params,\n",
        "            'lr': lr * concat_lr_ratio\n",
        "        }\n",
        "    ], lr = lr)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_valid_acc = 0\n",
        "    best_model = model\n",
        "    iters_since_best = 0\n",
        "\n",
        "    clip = 50.0\n",
        "    for epoch in tqdm.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "        with tqdm.tqdm(data_loader, desc=f\"epoch {epoch + 1}\", unit=\"batch\", total=len(data_loader), position=0, leave=True) as batch_iterator:\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "            for i, batch_data in enumerate(batch_iterator, start=1):\n",
        "                images, target = batch_data[\"image_tensors\"], batch_data[\"labels\"]\n",
        "                optimizer.zero_grad()\n",
        "                embeds, loss = model.compute_loss(images, target)\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping before taking the step\n",
        "                _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                optimizer.step()\n",
        "\n",
        "                batch_iterator.set_postfix(mean_loss=total_loss / i, current_loss=loss.item())\n",
        "\n",
        "            # compute validation accuracy and loss\n",
        "            model.eval()\n",
        "            valid_acc, valid_loss = make_classifications(model, num_classes, num_trials, valid_batch_size, valid_labels)\n",
        "            print(' %d-way, one-shot validation accuracy: %.1f%%. Validation loss: %f'%(num_classes, valid_acc*100, valid_loss))\n",
        "\n",
        "            if valid_acc > best_valid_acc:\n",
        "                best_valid_acc = valid_acc\n",
        "                iters_since_best = 0\n",
        "                best_model = copy.deepcopy(model)\n",
        "            else:\n",
        "                iters_since_best += 1\n",
        "\n",
        "            if iters_since_best >= 30:\n",
        "                break\n",
        "\n",
        "\n",
        "    # Perform final test and save the model after training\n",
        "    print('Final best validation accuracy: %.1f%%.'%(best_valid_acc*100))\n",
        "    test_acc, test_loss = make_classifications(best_model, num_classes, num_trials, valid_batch_size, test_labels)\n",
        "    print(' %d-way, one-shot test accuracy: %.1f%%. Test loss: %f'%(num_classes, test_acc*100, test_loss))\n",
        "    torch.save(best_model.state_dict(), model_file)\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfOxPVjfPyYF"
      },
      "source": [
        "## Training of Subtract and Concat Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUMGQyS8bjRj"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() # clears stuff from gpu\n",
        "\n",
        "# data params\n",
        "triplets = True\n",
        "num_triplets = int(1e5)\n",
        "num_same_pairs, num_diff_pairs = int(1e4), int(1e4)\n",
        "\n",
        "# sample the data\n",
        "if triplets:\n",
        "    images, indices = sample_triplets(num_triplets)\n",
        "else:\n",
        "    images, indices = sample_pairs(num_same_pairs, num_diff_pairs)\n",
        "\n",
        "# set up dataset and dataloader\n",
        "dataset = Omniglot_Dataset(images, indices, device, triplets=triplets)\n",
        "data_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aGPzBlPJBby"
      },
      "outputs": [],
      "source": [
        "# model params\n",
        "num_channels = [64, 128, 128, 256]\n",
        "kernel_sizes = [7, 5, 3, 3]\n",
        "pool_sizes = [2, 2, 2, 2]\n",
        "emb_dims = [4096, 1024]\n",
        "conv_dropout, linear_dropout = 0.2, 0.5\n",
        "\n",
        "# model = SiameseNet(num_channels, kernel_sizes, pool_sizes, emb_dims, conv_dropout, linear_dropout).to(device)\n",
        "model = SiameseNetConcat(num_channels, kernel_sizes, pool_sizes, emb_dims, conv_dropout, linear_dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfrQIZ1nXYbq",
        "outputId": "86a5a43b-a731-491f-a47a-43cace9a0331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triplets: 100000\n",
            "S-Concat\n",
            "23727873 parameters\n",
            " 5-way, one-shot accuracy: 20.3%. Loss: 0.350669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1: 100%|██████████| 3125/3125 [08:00<00:00,  6.50batch/s, current_loss=0.444, mean_loss=0.576]\n",
            "training:   0%|          | 1/200 [08:02<26:39:19, 482.21s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 71.6%. Validation loss: 0.379999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 2: 100%|██████████| 3125/3125 [08:02<00:00,  6.48batch/s, current_loss=0.434, mean_loss=0.468]\n",
            "training:   1%|          | 2/200 [16:05<26:34:20, 483.14s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 82.2%. Validation loss: 0.186675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 3: 100%|██████████| 3125/3125 [08:02<00:00,  6.48batch/s, current_loss=0.472, mean_loss=0.425]\n",
            "training:   2%|▏         | 3/200 [24:09<26:27:29, 483.50s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 88.1%. Validation loss: 0.324264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 4: 100%|██████████| 3125/3125 [08:01<00:00,  6.48batch/s, current_loss=0.271, mean_loss=0.401]\n",
            "training:   2%|▏         | 4/200 [32:13<26:19:26, 483.50s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 89.4%. Validation loss: 0.306855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 5: 100%|██████████| 3125/3125 [08:01<00:00,  6.50batch/s, current_loss=0.323, mean_loss=0.385]\n",
            "training:   2%|▎         | 5/200 [40:16<26:10:19, 483.18s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 87.2%. Validation loss: 0.268889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 6: 100%|██████████| 3125/3125 [08:01<00:00,  6.50batch/s, current_loss=0.514, mean_loss=0.373]\n",
            "training:   3%|▎         | 6/200 [48:18<26:01:42, 483.00s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 89.1%. Validation loss: 0.320250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 7: 100%|██████████| 3125/3125 [08:00<00:00,  6.50batch/s, current_loss=0.377, mean_loss=0.364]\n",
            "training:   4%|▎         | 7/200 [56:21<25:52:59, 482.80s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 85.0%. Validation loss: 0.335085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 8: 100%|██████████| 3125/3125 [08:01<00:00,  6.49batch/s, current_loss=0.342, mean_loss=0.354]\n",
            "training:   4%|▍         | 8/200 [1:04:23<25:44:51, 482.77s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 87.8%. Validation loss: 0.194210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 9: 100%|██████████| 3125/3125 [08:01<00:00,  6.50batch/s, current_loss=0.333, mean_loss=0.347]\n",
            "training:   4%|▍         | 9/200 [1:12:26<25:36:45, 482.75s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 92.2%. Validation loss: 0.331463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 10: 100%|██████████| 3125/3125 [08:02<00:00,  6.47batch/s, current_loss=0.355, mean_loss=0.339]\n",
            "training:   5%|▌         | 10/200 [1:20:30<25:30:13, 483.23s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 90.3%. Validation loss: 0.241578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 11: 100%|██████████| 3125/3125 [08:01<00:00,  6.49batch/s, current_loss=0.259, mean_loss=0.334]\n",
            "training:   6%|▌         | 11/200 [1:28:33<25:21:53, 483.14s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 90.9%. Validation loss: 0.261379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 12: 100%|██████████| 3125/3125 [08:01<00:00,  6.49batch/s, current_loss=0.326, mean_loss=0.328]\n",
            "training:   6%|▌         | 12/200 [1:36:36<25:13:56, 483.17s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 94.1%. Validation loss: 0.279219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 13: 100%|██████████| 3125/3125 [08:02<00:00,  6.48batch/s, current_loss=0.347, mean_loss=0.322]\n",
            "training:   6%|▋         | 13/200 [1:44:40<25:06:36, 483.40s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 92.8%. Validation loss: 0.225366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 14: 100%|██████████| 3125/3125 [08:02<00:00,  6.48batch/s, current_loss=0.344, mean_loss=0.32]\n",
            "training:   7%|▋         | 14/200 [1:52:44<24:59:10, 483.60s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 91.9%. Validation loss: 0.321050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 15: 100%|██████████| 3125/3125 [08:02<00:00,  6.48batch/s, current_loss=0.351, mean_loss=0.316]\n",
            "training:   8%|▊         | 15/200 [2:00:48<24:51:29, 483.73s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.9%. Validation loss: 0.222482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 16: 100%|██████████| 3125/3125 [08:01<00:00,  6.49batch/s, current_loss=0.352, mean_loss=0.314]\n",
            "training:   8%|▊         | 16/200 [2:08:52<24:43:00, 483.59s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.6%. Validation loss: 0.221295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 17: 100%|██████████| 3125/3125 [08:02<00:00,  6.48batch/s, current_loss=0.162, mean_loss=0.31]\n",
            "training:   8%|▊         | 17/200 [2:16:55<24:34:55, 483.58s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.3%. Validation loss: 0.312506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 18: 100%|██████████| 3125/3125 [08:05<00:00,  6.43batch/s, current_loss=0.403, mean_loss=0.307]\n",
            "training:   9%|▉         | 18/200 [2:25:03<24:30:14, 484.70s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 94.7%. Validation loss: 0.340916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 19: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.184, mean_loss=0.305]\n",
            "training:  10%|▉         | 19/200 [2:33:10<24:24:49, 485.58s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.2%. Validation loss: 0.158146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 20: 100%|██████████| 3125/3125 [08:05<00:00,  6.43batch/s, current_loss=0.404, mean_loss=0.302]\n",
            "training:  10%|█         | 20/200 [2:41:18<24:18:29, 486.17s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.9%. Validation loss: 0.396991\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 21: 100%|██████████| 3125/3125 [08:04<00:00,  6.44batch/s, current_loss=0.392, mean_loss=0.301]\n",
            "training:  10%|█         | 21/200 [2:49:24<24:10:44, 486.28s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.6%. Validation loss: 0.180501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 22: 100%|██████████| 3125/3125 [08:05<00:00,  6.44batch/s, current_loss=0.358, mean_loss=0.298]\n",
            "training:  11%|█         | 22/200 [2:57:31<24:03:14, 486.49s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.5%. Validation loss: 0.149447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 23: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.236, mean_loss=0.298]\n",
            "training:  12%|█▏        | 23/200 [3:05:39<23:56:10, 486.84s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 92.8%. Validation loss: 0.190563\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 24: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.227, mean_loss=0.297]\n",
            "training:  12%|█▏        | 24/200 [3:13:47<23:49:21, 487.28s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.9%. Validation loss: 0.392059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 25: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.422, mean_loss=0.294]\n",
            "training:  12%|█▎        | 25/200 [3:21:55<23:41:49, 487.48s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 98.4%. Validation loss: 0.134893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 26: 100%|██████████| 3125/3125 [08:07<00:00,  6.41batch/s, current_loss=0.359, mean_loss=0.291]\n",
            "training:  13%|█▎        | 26/200 [3:30:04<23:35:15, 488.02s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 98.4%. Validation loss: 0.132451\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 27: 100%|██████████| 3125/3125 [08:07<00:00,  6.41batch/s, current_loss=0.333, mean_loss=0.291]\n",
            "training:  14%|█▎        | 27/200 [3:38:13<23:27:58, 488.31s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 99.1%. Validation loss: 0.158971\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 28: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.202, mean_loss=0.289]\n",
            "training:  14%|█▍        | 28/200 [3:46:22<23:19:57, 488.36s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 98.1%. Validation loss: 0.169741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 29: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.346, mean_loss=0.291]\n",
            "training:  14%|█▍        | 29/200 [3:54:30<23:11:56, 488.40s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.2%. Validation loss: 0.174760\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 30: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.341, mean_loss=0.287]\n",
            "training:  15%|█▌        | 30/200 [4:02:38<23:03:24, 488.26s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.9%. Validation loss: 0.133090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 31: 100%|██████████| 3125/3125 [08:07<00:00,  6.41batch/s, current_loss=0.231, mean_loss=0.286]\n",
            "training:  16%|█▌        | 31/200 [4:10:47<22:55:38, 488.39s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.6%. Validation loss: 0.224862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 32: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.282, mean_loss=0.284]\n",
            "training:  16%|█▌        | 32/200 [4:18:55<22:47:21, 488.34s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.6%. Validation loss: 0.098161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 33: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.245, mean_loss=0.287]\n",
            "training:  16%|█▋        | 33/200 [4:27:03<22:38:58, 488.25s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 93.4%. Validation loss: 0.198669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 34: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.253, mean_loss=0.285]\n",
            "training:  17%|█▋        | 34/200 [4:35:12<22:31:01, 488.32s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.2%. Validation loss: 0.094090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 35: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.306, mean_loss=0.282]\n",
            "training:  18%|█▊        | 35/200 [4:43:20<22:22:32, 488.20s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.2%. Validation loss: 0.120643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 36: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.272, mean_loss=0.282]\n",
            "training:  18%|█▊        | 36/200 [4:51:28<22:14:35, 488.27s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.6%. Validation loss: 0.164893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 37: 100%|██████████| 3125/3125 [08:07<00:00,  6.41batch/s, current_loss=0.29, mean_loss=0.281]\n",
            "training:  18%|█▊        | 37/200 [4:59:37<22:07:01, 488.47s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.8%. Validation loss: 0.195291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 38: 100%|██████████| 3125/3125 [08:07<00:00,  6.42batch/s, current_loss=0.201, mean_loss=0.281]\n",
            "training:  19%|█▉        | 38/200 [5:07:46<21:58:59, 488.52s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 98.1%. Validation loss: 0.099972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 39: 100%|██████████| 3125/3125 [08:07<00:00,  6.41batch/s, current_loss=0.305, mean_loss=0.281]\n",
            "training:  20%|█▉        | 39/200 [5:15:55<21:51:08, 488.62s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.3%. Validation loss: 0.312684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 40: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.314, mean_loss=0.279]\n",
            "training:  20%|██        | 40/200 [5:24:03<21:42:33, 488.46s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.3%. Validation loss: 0.150551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 41: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.249, mean_loss=0.278]\n",
            "training:  20%|██        | 41/200 [5:32:11<21:34:26, 488.47s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.6%. Validation loss: 0.139179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 42: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.291, mean_loss=0.278]\n",
            "training:  21%|██        | 42/200 [5:40:19<21:26:08, 488.41s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.9%. Validation loss: 0.179681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 43: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.304, mean_loss=0.278]\n",
            "training:  22%|██▏       | 43/200 [5:48:27<21:17:39, 488.28s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.2%. Validation loss: 0.156739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 44: 100%|██████████| 3125/3125 [08:05<00:00,  6.43batch/s, current_loss=0.359, mean_loss=0.278]\n",
            "training:  22%|██▏       | 44/200 [5:56:35<21:08:55, 488.05s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.9%. Validation loss: 0.129933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 45: 100%|██████████| 3125/3125 [08:05<00:00,  6.43batch/s, current_loss=0.186, mean_loss=0.276]\n",
            "training:  22%|██▎       | 45/200 [6:04:42<21:00:21, 487.88s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.5%. Validation loss: 0.165898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 46: 100%|██████████| 3125/3125 [08:05<00:00,  6.43batch/s, current_loss=0.214, mean_loss=0.277]\n",
            "training:  23%|██▎       | 46/200 [6:12:50<20:51:52, 487.74s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 98.8%. Validation loss: 0.075916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 47: 100%|██████████| 3125/3125 [08:05<00:00,  6.44batch/s, current_loss=0.288, mean_loss=0.278]\n",
            "training:  24%|██▎       | 47/200 [6:20:57<20:43:15, 487.55s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.5%. Validation loss: 0.106281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 48: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.245, mean_loss=0.276]\n",
            "training:  24%|██▍       | 48/200 [6:29:05<20:35:29, 487.69s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.6%. Validation loss: 0.072471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 49: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.224, mean_loss=0.274]\n",
            "training:  24%|██▍       | 49/200 [6:37:13<20:27:27, 487.73s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.5%. Validation loss: 0.137844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 50: 100%|██████████| 3125/3125 [08:05<00:00,  6.44batch/s, current_loss=0.263, mean_loss=0.274]\n",
            "training:  25%|██▌       | 50/200 [6:45:20<20:18:45, 487.50s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 92.8%. Validation loss: 0.192448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 51: 100%|██████████| 3125/3125 [08:05<00:00,  6.43batch/s, current_loss=0.235, mean_loss=0.274]\n",
            "training:  26%|██▌       | 51/200 [6:53:27<20:10:26, 487.42s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.8%. Validation loss: 0.098805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 52: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.3, mean_loss=0.273]\n",
            "training:  26%|██▌       | 52/200 [7:01:35<20:02:26, 487.47s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.5%. Validation loss: 0.065325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 53: 100%|██████████| 3125/3125 [08:05<00:00,  6.44batch/s, current_loss=0.326, mean_loss=0.273]\n",
            "training:  26%|██▋       | 53/200 [7:09:42<19:54:03, 487.37s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 97.8%. Validation loss: 0.092698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 54: 100%|██████████| 3125/3125 [08:06<00:00,  6.42batch/s, current_loss=0.236, mean_loss=0.273]\n",
            "training:  27%|██▋       | 54/200 [7:17:50<19:46:22, 487.55s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 98.1%. Validation loss: 0.116222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 55: 100%|██████████| 3125/3125 [08:05<00:00,  6.44batch/s, current_loss=0.211, mean_loss=0.274]\n",
            "training:  28%|██▊       | 55/200 [7:25:57<19:37:45, 487.35s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.6%. Validation loss: 0.159826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 56: 100%|██████████| 3125/3125 [08:05<00:00,  6.43batch/s, current_loss=0.262, mean_loss=0.273]\n",
            "training:  28%|██▊       | 56/200 [7:34:04<19:29:33, 487.32s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 95.3%. Validation loss: 0.114390\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 57: 100%|██████████| 3125/3125 [08:06<00:00,  6.43batch/s, current_loss=0.283, mean_loss=0.272]\n",
            "training:  28%|██▊       | 56/200 [7:42:11<19:48:30, 495.21s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot validation accuracy: 96.6%. Validation loss: 0.078911\n",
            "Final best validation accuracy: 99.1%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 5-way, one-shot test accuracy: 90.3%. Test loss: 0.310094\n"
          ]
        }
      ],
      "source": [
        "# print model details\n",
        "if triplets:\n",
        "    print(\"Triplets:\", num_triplets)\n",
        "else:\n",
        "    print(\"Pairs:\", num_same_pairs, num_diff_pairs)\n",
        "print(['S-Subtract','S-Concat'][int(isinstance(model, SiameseNetConcat))])\n",
        "\n",
        "# calculate number of parameters\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(params, 'parameters')\n",
        "\n",
        "#train\n",
        "best_model = train(model, data_loader, \"siamese_model.pt\", lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaslHFMtGwAW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download('siamese_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2S18hWQKHTw"
      },
      "source": [
        "## S-Multires training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Bop79HO0Vr1"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() # clears stuff from gpu\n",
        "\n",
        "# model params\n",
        "triplets = True\n",
        "num_triplets = int(1e4)\n",
        "num_same_pairs, num_diff_pairs = int(1e4), int(1e4)\n",
        "\n",
        "if triplets:\n",
        "    triplets, indices = sample_triplets(num_triplets)\n",
        "    dataset = Omniglot_Dataset(triplets, indices, device, triplets=triplets)\n",
        "else:\n",
        "    # sampling pairs\n",
        "    pairs, indices = sample_pairs(num_same_pairs, num_diff_pairs)\n",
        "    # set up dataset\n",
        "    dataset = Omniglot_Dataset(pairs, indices, device, triplets=triplets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XSAcKQVcjpL",
        "outputId": "8a0e47a5-c4ed-4fd3-a3fa-df5ddb3f1cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6022945 parameters\n"
          ]
        }
      ],
      "source": [
        "# train params\n",
        "max_num_epochs = 200\n",
        "batch_size = 64\n",
        "\n",
        "# model params\n",
        "num_conv_layers = 8\n",
        "conv_dropout, linear_dropout = 0.2, 0.5\n",
        "\n",
        "# accuracy test params\n",
        "num_classes = 5\n",
        "num_tests = 320\n",
        "\n",
        "# set up dataloader and model\n",
        "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "multires = MultiResNet(num_conv_layers, conv_dropout, linear_dropout).to(device)\n",
        "\n",
        "# calculate number of parameters\n",
        "model_parameters = filter(lambda p: p.requires_grad, multires.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(params, 'parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3P3twMkcj98"
      },
      "outputs": [],
      "source": [
        "#Testing cell - delete later\n",
        "best_model = train(multires, data_loader, \"siamese_model_multires.pt\", lr=1e-5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Q2S18hWQKHTw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}